{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anil1331/EIP3-Assignments/blob/master/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boTqjD3P6qlt",
        "colab_type": "code",
        "outputId": "870f0364-baf0-40b4-9ad8-f34621ea69ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwMwmv2j62Jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp drive/'My Drive'/wonderland.txt ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHeC2ASy7uOQ",
        "colab_type": "code",
        "outputId": "9ca06e35-74e3-4ea3-ba11-a1cbdf042c58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68KjSipm7-l7",
        "colab_type": "code",
        "outputId": "032ff287-1f8f-4fc9-ef01-59da466b38f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "raw_text[:1000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'alices adventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 30\\n\\n\\n\\n\\nchapter i down the rabbithole\\n\\nalice was beginning to get very tired of sitting by her sister on the\\nbank and of having nothing to do once or twice she had peeped into the\\nbook her sister was reading but it had no pictures or conversations in\\nit and what is the use of a book thought alice without pictures or\\nconversations\\n\\nso she was considering in her own mind as well as she could for the\\nhot day made her feel very sleepy and stupid whether the pleasure\\nof making a daisychain would be worth the trouble of getting up and\\npicking the daisies when suddenly a white rabbit with pink eyes ran\\nclose by her\\n\\nthere was nothing so very remarkable in that nor did alice think it so\\nvery much out of the way to hear the rabbit say to itself oh dear\\noh dear i shall be late when she thought it over afterwards it\\noccurred to her that she ought to have wondered at this but at the time\\nit all seemed quite natural but w'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zozRTjEoNqKZ",
        "colab_type": "text"
      },
      "source": [
        "Removed Punctuation from source text for better results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rVNZgn18D5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "raw_text = re.sub(r'[^\\w\\s]','',raw_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0Q-DV3J8PRE",
        "colab_type": "code",
        "outputId": "d674c1a2-fa41-4511-99f2-6e3afa476440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "chars"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '0',\n",
              " '3',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wynE3v8l6dLN",
        "colab_type": "code",
        "outputId": "fb845f95-f694-47ad-e522-68788da802d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  136091\n",
            "Total Vocab:  31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDNI9nxoF_NV",
        "colab_type": "text"
      },
      "source": [
        "Article is split by sentences and each set of 100 characters in a sentence is used for training. If the length of sentence is less than 100 it is **padded** using keras **pad_sequences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c1IVYOb90nA",
        "colab_type": "code",
        "outputId": "92f4cf05-b12b-43f8-a84e-b2cf341ce160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "sentence = raw_text.split(\"\\n\\n\")\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "sent = ([x for x in sentence if sum(c.isalpha() for c in x)>1])\n",
        "dataX = []\n",
        "dataY = []\n",
        "seq_length = 101\n",
        "for s in sent:\n",
        "  if len(s)<101:\n",
        "    x = [char_to_int[a] for a in s]\n",
        "    x = pad_sequences([x], maxlen=seq_length)\n",
        "    dataX.append([a for a in x[0][:-1]])\n",
        "    dataY.append(x[0][-1])\n",
        "  else:\n",
        "    for i in range(0, len(s) - seq_length, 1):\n",
        "      x = [char_to_int[a] for a in s[i:i + seq_length]] \n",
        "      dataX.append([a for a in x[:-1]])\n",
        "      dataY.append(x[-1])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "print(len(dataX))\n",
        "X = numpy.reshape(numpy.array(dataX), (n_patterns, seq_length-1, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  68970\n",
            "68970\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGeEgohz8iEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aJMW2sh8kqE",
        "colab_type": "code",
        "outputId": "d7cd3411-1c8e-4723-b255-33611da82b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "68970/68970 [==============================] - 118s 2ms/step - loss: 2.8818\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.88184, saving model to weights-improvement-01-2.8818-bigger.hdf5\n",
            "Epoch 2/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 2.8148\n",
            "\n",
            "Epoch 00002: loss improved from 2.88184 to 2.81480, saving model to weights-improvement-02-2.8148-bigger.hdf5\n",
            "Epoch 3/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 2.7083\n",
            "\n",
            "Epoch 00003: loss improved from 2.81480 to 2.70829, saving model to weights-improvement-03-2.7083-bigger.hdf5\n",
            "Epoch 4/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 2.6400\n",
            "\n",
            "Epoch 00004: loss improved from 2.70829 to 2.63997, saving model to weights-improvement-04-2.6400-bigger.hdf5\n",
            "Epoch 5/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 2.5608\n",
            "\n",
            "Epoch 00005: loss improved from 2.63997 to 2.56079, saving model to weights-improvement-05-2.5608-bigger.hdf5\n",
            "Epoch 6/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 2.4753\n",
            "\n",
            "Epoch 00006: loss improved from 2.56079 to 2.47525, saving model to weights-improvement-06-2.4753-bigger.hdf5\n",
            "Epoch 7/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 2.3977\n",
            "\n",
            "Epoch 00007: loss improved from 2.47525 to 2.39767, saving model to weights-improvement-07-2.3977-bigger.hdf5\n",
            "Epoch 8/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 2.3332\n",
            "\n",
            "Epoch 00008: loss improved from 2.39767 to 2.33317, saving model to weights-improvement-08-2.3332-bigger.hdf5\n",
            "Epoch 9/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 2.2832\n",
            "\n",
            "Epoch 00009: loss improved from 2.33317 to 2.28318, saving model to weights-improvement-09-2.2832-bigger.hdf5\n",
            "Epoch 10/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 2.2319\n",
            "\n",
            "Epoch 00010: loss improved from 2.28318 to 2.23191, saving model to weights-improvement-10-2.2319-bigger.hdf5\n",
            "Epoch 11/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 2.1987\n",
            "\n",
            "Epoch 00011: loss improved from 2.23191 to 2.19868, saving model to weights-improvement-11-2.1987-bigger.hdf5\n",
            "Epoch 12/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 2.1531\n",
            "\n",
            "Epoch 00012: loss improved from 2.19868 to 2.15313, saving model to weights-improvement-12-2.1531-bigger.hdf5\n",
            "Epoch 13/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 2.1197\n",
            "\n",
            "Epoch 00013: loss improved from 2.15313 to 2.11967, saving model to weights-improvement-13-2.1197-bigger.hdf5\n",
            "Epoch 14/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 2.0932\n",
            "\n",
            "Epoch 00014: loss improved from 2.11967 to 2.09324, saving model to weights-improvement-14-2.0932-bigger.hdf5\n",
            "Epoch 15/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 2.0619\n",
            "\n",
            "Epoch 00015: loss improved from 2.09324 to 2.06186, saving model to weights-improvement-15-2.0619-bigger.hdf5\n",
            "Epoch 16/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 2.0355\n",
            "\n",
            "Epoch 00016: loss improved from 2.06186 to 2.03548, saving model to weights-improvement-16-2.0355-bigger.hdf5\n",
            "Epoch 17/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 2.0125\n",
            "\n",
            "Epoch 00017: loss improved from 2.03548 to 2.01245, saving model to weights-improvement-17-2.0125-bigger.hdf5\n",
            "Epoch 18/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.9866\n",
            "\n",
            "Epoch 00018: loss improved from 2.01245 to 1.98663, saving model to weights-improvement-18-1.9866-bigger.hdf5\n",
            "Epoch 19/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.9639\n",
            "\n",
            "Epoch 00019: loss improved from 1.98663 to 1.96394, saving model to weights-improvement-19-1.9639-bigger.hdf5\n",
            "Epoch 20/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.9459\n",
            "\n",
            "Epoch 00020: loss improved from 1.96394 to 1.94589, saving model to weights-improvement-20-1.9459-bigger.hdf5\n",
            "Epoch 21/100\n",
            "68970/68970 [==============================] - 118s 2ms/step - loss: 1.9212\n",
            "\n",
            "Epoch 00021: loss improved from 1.94589 to 1.92116, saving model to weights-improvement-21-1.9212-bigger.hdf5\n",
            "Epoch 22/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.9081\n",
            "\n",
            "Epoch 00022: loss improved from 1.92116 to 1.90811, saving model to weights-improvement-22-1.9081-bigger.hdf5\n",
            "Epoch 23/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.8878\n",
            "\n",
            "Epoch 00023: loss improved from 1.90811 to 1.88780, saving model to weights-improvement-23-1.8878-bigger.hdf5\n",
            "Epoch 24/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.8763\n",
            "\n",
            "Epoch 00024: loss improved from 1.88780 to 1.87635, saving model to weights-improvement-24-1.8763-bigger.hdf5\n",
            "Epoch 25/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.8638\n",
            "\n",
            "Epoch 00025: loss improved from 1.87635 to 1.86384, saving model to weights-improvement-25-1.8638-bigger.hdf5\n",
            "Epoch 26/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.8403\n",
            "\n",
            "Epoch 00026: loss improved from 1.86384 to 1.84028, saving model to weights-improvement-26-1.8403-bigger.hdf5\n",
            "Epoch 27/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.8288\n",
            "\n",
            "Epoch 00027: loss improved from 1.84028 to 1.82883, saving model to weights-improvement-27-1.8288-bigger.hdf5\n",
            "Epoch 28/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 1.8179\n",
            "\n",
            "Epoch 00028: loss improved from 1.82883 to 1.81786, saving model to weights-improvement-28-1.8179-bigger.hdf5\n",
            "Epoch 29/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 1.7977\n",
            "\n",
            "Epoch 00029: loss improved from 1.81786 to 1.79772, saving model to weights-improvement-29-1.7977-bigger.hdf5\n",
            "Epoch 30/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 1.7824\n",
            "\n",
            "Epoch 00030: loss improved from 1.79772 to 1.78236, saving model to weights-improvement-30-1.7824-bigger.hdf5\n",
            "Epoch 31/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.7746\n",
            "\n",
            "Epoch 00031: loss improved from 1.78236 to 1.77460, saving model to weights-improvement-31-1.7746-bigger.hdf5\n",
            "Epoch 32/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.7532\n",
            "\n",
            "Epoch 00032: loss improved from 1.77460 to 1.75320, saving model to weights-improvement-32-1.7532-bigger.hdf5\n",
            "Epoch 33/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.7467\n",
            "\n",
            "Epoch 00033: loss improved from 1.75320 to 1.74669, saving model to weights-improvement-33-1.7467-bigger.hdf5\n",
            "Epoch 34/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.7379\n",
            "\n",
            "Epoch 00034: loss improved from 1.74669 to 1.73788, saving model to weights-improvement-34-1.7379-bigger.hdf5\n",
            "Epoch 35/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.7273\n",
            "\n",
            "Epoch 00035: loss improved from 1.73788 to 1.72734, saving model to weights-improvement-35-1.7273-bigger.hdf5\n",
            "Epoch 36/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.7133\n",
            "\n",
            "Epoch 00036: loss improved from 1.72734 to 1.71334, saving model to weights-improvement-36-1.7133-bigger.hdf5\n",
            "Epoch 37/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.7019\n",
            "\n",
            "Epoch 00037: loss improved from 1.71334 to 1.70190, saving model to weights-improvement-37-1.7019-bigger.hdf5\n",
            "Epoch 38/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.6914\n",
            "\n",
            "Epoch 00038: loss improved from 1.70190 to 1.69145, saving model to weights-improvement-38-1.6914-bigger.hdf5\n",
            "Epoch 39/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.6825\n",
            "\n",
            "Epoch 00039: loss improved from 1.69145 to 1.68247, saving model to weights-improvement-39-1.6825-bigger.hdf5\n",
            "Epoch 40/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.6668\n",
            "\n",
            "Epoch 00040: loss improved from 1.68247 to 1.66681, saving model to weights-improvement-40-1.6668-bigger.hdf5\n",
            "Epoch 41/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.6632\n",
            "\n",
            "Epoch 00041: loss improved from 1.66681 to 1.66325, saving model to weights-improvement-41-1.6632-bigger.hdf5\n",
            "Epoch 42/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.6566\n",
            "\n",
            "Epoch 00042: loss improved from 1.66325 to 1.65663, saving model to weights-improvement-42-1.6566-bigger.hdf5\n",
            "Epoch 43/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.6416\n",
            "\n",
            "Epoch 00043: loss improved from 1.65663 to 1.64161, saving model to weights-improvement-43-1.6416-bigger.hdf5\n",
            "Epoch 44/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.6332\n",
            "\n",
            "Epoch 00044: loss improved from 1.64161 to 1.63316, saving model to weights-improvement-44-1.6332-bigger.hdf5\n",
            "Epoch 45/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.6222\n",
            "\n",
            "Epoch 00045: loss improved from 1.63316 to 1.62219, saving model to weights-improvement-45-1.6222-bigger.hdf5\n",
            "Epoch 46/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.6162\n",
            "\n",
            "Epoch 00046: loss improved from 1.62219 to 1.61618, saving model to weights-improvement-46-1.6162-bigger.hdf5\n",
            "Epoch 47/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.6044\n",
            "\n",
            "Epoch 00047: loss improved from 1.61618 to 1.60435, saving model to weights-improvement-47-1.6044-bigger.hdf5\n",
            "Epoch 48/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.6004\n",
            "\n",
            "Epoch 00048: loss improved from 1.60435 to 1.60036, saving model to weights-improvement-48-1.6004-bigger.hdf5\n",
            "Epoch 49/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.6001\n",
            "\n",
            "Epoch 00049: loss improved from 1.60036 to 1.60015, saving model to weights-improvement-49-1.6001-bigger.hdf5\n",
            "Epoch 50/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.5843\n",
            "\n",
            "Epoch 00050: loss improved from 1.60015 to 1.58426, saving model to weights-improvement-50-1.5843-bigger.hdf5\n",
            "Epoch 51/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.5791\n",
            "\n",
            "Epoch 00051: loss improved from 1.58426 to 1.57914, saving model to weights-improvement-51-1.5791-bigger.hdf5\n",
            "Epoch 52/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.5640\n",
            "\n",
            "Epoch 00052: loss improved from 1.57914 to 1.56399, saving model to weights-improvement-52-1.5640-bigger.hdf5\n",
            "Epoch 53/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.5546\n",
            "\n",
            "Epoch 00053: loss improved from 1.56399 to 1.55463, saving model to weights-improvement-53-1.5546-bigger.hdf5\n",
            "Epoch 54/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.5479\n",
            "\n",
            "Epoch 00054: loss improved from 1.55463 to 1.54792, saving model to weights-improvement-54-1.5479-bigger.hdf5\n",
            "Epoch 55/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.5442\n",
            "\n",
            "Epoch 00055: loss improved from 1.54792 to 1.54424, saving model to weights-improvement-55-1.5442-bigger.hdf5\n",
            "Epoch 56/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.5336\n",
            "\n",
            "Epoch 00056: loss improved from 1.54424 to 1.53362, saving model to weights-improvement-56-1.5336-bigger.hdf5\n",
            "Epoch 57/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.5338\n",
            "\n",
            "Epoch 00057: loss did not improve from 1.53362\n",
            "Epoch 58/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.5223\n",
            "\n",
            "Epoch 00058: loss improved from 1.53362 to 1.52226, saving model to weights-improvement-58-1.5223-bigger.hdf5\n",
            "Epoch 59/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.5171\n",
            "\n",
            "Epoch 00059: loss improved from 1.52226 to 1.51706, saving model to weights-improvement-59-1.5171-bigger.hdf5\n",
            "Epoch 60/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 1.5175\n",
            "\n",
            "Epoch 00060: loss did not improve from 1.51706\n",
            "Epoch 61/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 1.5081\n",
            "\n",
            "Epoch 00061: loss improved from 1.51706 to 1.50809, saving model to weights-improvement-61-1.5081-bigger.hdf5\n",
            "Epoch 62/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.4965\n",
            "\n",
            "Epoch 00062: loss improved from 1.50809 to 1.49649, saving model to weights-improvement-62-1.4965-bigger.hdf5\n",
            "Epoch 63/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.4913\n",
            "\n",
            "Epoch 00063: loss improved from 1.49649 to 1.49129, saving model to weights-improvement-63-1.4913-bigger.hdf5\n",
            "Epoch 64/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.4847\n",
            "\n",
            "Epoch 00064: loss improved from 1.49129 to 1.48467, saving model to weights-improvement-64-1.4847-bigger.hdf5\n",
            "Epoch 65/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.4825\n",
            "\n",
            "Epoch 00065: loss improved from 1.48467 to 1.48249, saving model to weights-improvement-65-1.4825-bigger.hdf5\n",
            "Epoch 66/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.4774\n",
            "\n",
            "Epoch 00066: loss improved from 1.48249 to 1.47738, saving model to weights-improvement-66-1.4774-bigger.hdf5\n",
            "Epoch 67/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.4682\n",
            "\n",
            "Epoch 00067: loss improved from 1.47738 to 1.46819, saving model to weights-improvement-67-1.4682-bigger.hdf5\n",
            "Epoch 68/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.4636\n",
            "\n",
            "Epoch 00068: loss improved from 1.46819 to 1.46361, saving model to weights-improvement-68-1.4636-bigger.hdf5\n",
            "Epoch 69/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.4540\n",
            "\n",
            "Epoch 00069: loss improved from 1.46361 to 1.45395, saving model to weights-improvement-69-1.4540-bigger.hdf5\n",
            "Epoch 70/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.4478\n",
            "\n",
            "Epoch 00070: loss improved from 1.45395 to 1.44784, saving model to weights-improvement-70-1.4478-bigger.hdf5\n",
            "Epoch 71/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.4493\n",
            "\n",
            "Epoch 00071: loss did not improve from 1.44784\n",
            "Epoch 72/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.4456\n",
            "\n",
            "Epoch 00072: loss improved from 1.44784 to 1.44561, saving model to weights-improvement-72-1.4456-bigger.hdf5\n",
            "Epoch 73/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.4393\n",
            "\n",
            "Epoch 00073: loss improved from 1.44561 to 1.43932, saving model to weights-improvement-73-1.4393-bigger.hdf5\n",
            "Epoch 74/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 1.4277\n",
            "\n",
            "Epoch 00074: loss improved from 1.43932 to 1.42767, saving model to weights-improvement-74-1.4277-bigger.hdf5\n",
            "Epoch 75/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.4217\n",
            "\n",
            "Epoch 00075: loss improved from 1.42767 to 1.42172, saving model to weights-improvement-75-1.4217-bigger.hdf5\n",
            "Epoch 76/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.4108\n",
            "\n",
            "Epoch 00076: loss improved from 1.42172 to 1.41076, saving model to weights-improvement-76-1.4108-bigger.hdf5\n",
            "Epoch 77/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.4142\n",
            "\n",
            "Epoch 00077: loss did not improve from 1.41076\n",
            "Epoch 78/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.4112\n",
            "\n",
            "Epoch 00078: loss did not improve from 1.41076\n",
            "Epoch 79/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.4032\n",
            "\n",
            "Epoch 00079: loss improved from 1.41076 to 1.40323, saving model to weights-improvement-79-1.4032-bigger.hdf5\n",
            "Epoch 80/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.3980\n",
            "\n",
            "Epoch 00080: loss improved from 1.40323 to 1.39801, saving model to weights-improvement-80-1.3980-bigger.hdf5\n",
            "Epoch 81/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.3950\n",
            "\n",
            "Epoch 00081: loss improved from 1.39801 to 1.39501, saving model to weights-improvement-81-1.3950-bigger.hdf5\n",
            "Epoch 82/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.3824\n",
            "\n",
            "Epoch 00082: loss improved from 1.39501 to 1.38242, saving model to weights-improvement-82-1.3824-bigger.hdf5\n",
            "Epoch 83/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.3807\n",
            "\n",
            "Epoch 00083: loss improved from 1.38242 to 1.38069, saving model to weights-improvement-83-1.3807-bigger.hdf5\n",
            "Epoch 84/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.3893\n",
            "\n",
            "Epoch 00084: loss did not improve from 1.38069\n",
            "Epoch 85/100\n",
            "68970/68970 [==============================] - 117s 2ms/step - loss: 1.3724\n",
            "\n",
            "Epoch 00085: loss improved from 1.38069 to 1.37244, saving model to weights-improvement-85-1.3724-bigger.hdf5\n",
            "Epoch 86/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.3681\n",
            "\n",
            "Epoch 00086: loss improved from 1.37244 to 1.36813, saving model to weights-improvement-86-1.3681-bigger.hdf5\n",
            "Epoch 87/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.3715\n",
            "\n",
            "Epoch 00087: loss did not improve from 1.36813\n",
            "Epoch 88/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.3643\n",
            "\n",
            "Epoch 00088: loss improved from 1.36813 to 1.36428, saving model to weights-improvement-88-1.3643-bigger.hdf5\n",
            "Epoch 89/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.3579\n",
            "\n",
            "Epoch 00089: loss improved from 1.36428 to 1.35794, saving model to weights-improvement-89-1.3579-bigger.hdf5\n",
            "Epoch 90/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.3590\n",
            "\n",
            "Epoch 00090: loss did not improve from 1.35794\n",
            "Epoch 91/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.3526\n",
            "\n",
            "Epoch 00091: loss improved from 1.35794 to 1.35262, saving model to weights-improvement-91-1.3526-bigger.hdf5\n",
            "Epoch 92/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.3446\n",
            "\n",
            "Epoch 00092: loss improved from 1.35262 to 1.34465, saving model to weights-improvement-92-1.3446-bigger.hdf5\n",
            "Epoch 93/100\n",
            "68970/68970 [==============================] - 116s 2ms/step - loss: 1.3425\n",
            "\n",
            "Epoch 00093: loss improved from 1.34465 to 1.34250, saving model to weights-improvement-93-1.3425-bigger.hdf5\n",
            "Epoch 94/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.3342\n",
            "\n",
            "Epoch 00094: loss improved from 1.34250 to 1.33421, saving model to weights-improvement-94-1.3342-bigger.hdf5\n",
            "Epoch 95/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.3399\n",
            "\n",
            "Epoch 00095: loss did not improve from 1.33421\n",
            "Epoch 96/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.3358\n",
            "\n",
            "Epoch 00096: loss did not improve from 1.33421\n",
            "Epoch 97/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.3329\n",
            "\n",
            "Epoch 00097: loss improved from 1.33421 to 1.33288, saving model to weights-improvement-97-1.3329-bigger.hdf5\n",
            "Epoch 98/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.3221\n",
            "\n",
            "Epoch 00098: loss improved from 1.33288 to 1.32205, saving model to weights-improvement-98-1.3221-bigger.hdf5\n",
            "Epoch 99/100\n",
            "68970/68970 [==============================] - 114s 2ms/step - loss: 1.3172\n",
            "\n",
            "Epoch 00099: loss improved from 1.32205 to 1.31721, saving model to weights-improvement-99-1.3172-bigger.hdf5\n",
            "Epoch 100/100\n",
            "68970/68970 [==============================] - 115s 2ms/step - loss: 1.3131\n",
            "\n",
            "Epoch 00100: loss improved from 1.31721 to 1.31308, saving model to weights-improvement-100-1.3131-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5d2bb288d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRd4RakZ8tPm",
        "colab_type": "code",
        "outputId": "9ba5dcc6-4c04-4fc1-b420-384a929063cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import sys\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" ant the king said turning to the jury they were\n",
            "just beginning to write this down on their slates wh \"\n",
            "ar tueer lls tenemberer i an io tie wooold to the whe polee of teach whe suean whe gatdr anlce sectidd ooch oarty anlce whe eoow suier siolle bnice whe polent thalint anice coulde of gettet alice began toall be a large and tooetneer what suehcat sfmeiicered tound and look d ano turt i tuappee ier anice cooidd awatp yhat iad mapied becurifat ofx anai iopedi and teadhinid iir head tevplces k onled at all oergatunane yhich satpintsy anice teadi tomething anice loowes at ier tie whe polehteoed ou te\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN6B3N8AEnVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}